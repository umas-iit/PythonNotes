{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "oop-Python-WebscrapingProject-BS-GC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umas-iit/PythonNotes/blob/main/oop_Python_WebscrapingProject_BS_GC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcf2cNmvhiP3"
      },
      "source": [
        "## Web Scrapping with Python  and Beautiful Soup:\n",
        "\n",
        "Amount of data available on the Internet is a rich resource for any type of research or personal data analysis . To effectively harvest the data, a techinque called web scraping is used. The Python libraries requests and Beautiful Soup are powerful web scraping tools for the job.\n",
        "Use requests and Beautiful Soup for scraping and parsing data from the Web example is shown below \n",
        "\n",
        "### What Is Web Scraping?\n",
        "Web scraping is the process of gathering information from the Internet. The word “web scraping” usually refer to a process that involves automation.Automated web scraping can be a solution to speed up the data collection process. Manual web scraping can take a lot of time and repetition.\n",
        "Because the internet is dynamic, the scrapers we use will probably require constant maintenance. It is always useful to  set up continuous integration to run scraping tests periodically to ensure that the main script doesn’t break without our  knowledge.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjyCkv-ZkUr7"
      },
      "source": [
        "### Scrape Static HTML Content From a Page\n",
        "* Get the site’s HTML code into your Python script so that you can interact  with it. \n",
        "* use Python’s requests library. \n",
        "\n",
        "This code performs an HTTP request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object.\n",
        "The server that hosts the site sends back HTML documents that already contain all the data \n",
        "\n",
        "The Components of a Web Page:\n",
        "When we visit a web page, our web browser makes a request to a web server. This request is called a GET request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. These files will typically include:\n",
        " *  HTML — the main content of the page. HTML consists of elements called tags. The most basic tag is the <html> tag. This tag tells the web browser that everything inside of it is HTML.\n",
        " *  CSS — used to add styling to make the page look nicer.\n",
        " *  JS — Javascript files add interactivity to web pages.\n",
        " *  Images — image formats, such as JPG and PNG, allow web pages to show pictures.\n",
        "After our browser receives all the files, it renders the page and displays it to us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRHbZkj7V1Fc"
      },
      "source": [
        "# HTML page example\n",
        "\n",
        "<html>\n",
        "<head>\n",
        "</head>\n",
        "<body>\n",
        "<p>\n",
        "Here's a paragraph of text!\n",
        "<a href=\"https://www.dataquest.io\">Learn Data Science Online</a>\n",
        "</p>\n",
        "<p>\n",
        "Here's a second paragraph of text!\n",
        "<a href=\"https://www.python.org\">Python</a> </p>\n",
        "</body></html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J__kq3bHrxfK"
      },
      "source": [
        "# Example 1:  The requests library\n",
        "#  A python code without pprint\n",
        "import requests\n",
        "  \n",
        "def geocode(address):\n",
        "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
        "    resp = requests.get(url, params = {'address': address})\n",
        "    return resp.json()\n",
        "  \n",
        "# calling geocode function\n",
        "data = geocode('India gate')\n",
        "\n",
        "# printing json response\n",
        "print(data)\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "page = requests.get(\"https://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
        "page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-s8pvLys58Z"
      },
      "source": [
        "# Example 2:  A python code with pprint\n",
        "import requests\n",
        "from pprint import pprint\n",
        "  \n",
        "def geocode(address):\n",
        "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
        "    resp = requests.get(url, params = {'address': address})\n",
        "    return resp.json()\n",
        "  \n",
        "# calling geocode function\n",
        "data = geocode('India gate')\n",
        "  \n",
        "# printing json response\n",
        "pprint(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaTSh-EEvBwJ"
      },
      "source": [
        "### Techniques  :\n",
        "Get all of the data from inside a table that was displayed on a web page.\n",
        "steps in sequence:\n",
        "* Request the content (source code) of a specific URL from the server\n",
        "* Download the content that is returned\n",
        "* Identify the elements of the page that are part of the table we want\n",
        "* Extract and (if necessary) reformat those elements into a dataset we can analyze or use in whatever way we require."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAKfvDIKSjMO"
      },
      "source": [
        "#. https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/\n",
        "\n",
        "import requests\n",
        "#page = requests.get(\"https://github.com/umas-iit/Python-Examples/Pythonsample1.html\")\n",
        "page = requests.get(\"https://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
        "page\n",
        "#print(page.status_code)\n",
        "#print(page.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVi1zppBvtg8"
      },
      "source": [
        "# Parsing a page with BeautifulSoup\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "print(soup.prettify())\n",
        "list(soup.children)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0acxHDGHVc9a"
      },
      "source": [
        "### All of the items are BeautifulSoup objects:\n",
        "# The first is a Doctype object, which contains information about the type of the document.\n",
        "# The second is a NavigableString, which represents text found in the HTML document.\n",
        "# The final item is a Tag object, which contains other nested tags.he Tag object allows \n",
        "# us to navigate through an HTML document, and extract other tags and text.\n",
        "\n",
        "[type(item) for item in list(soup.children)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B32z1dVVc9a"
      },
      "source": [
        "html = list(soup.children)[2]      # select the html tag and its children by taking the third item in the list\n",
        "list(html.children)\n",
        "body = list(html.children)[3]\n",
        "list(body.children)\n",
        "p = list(body.children)[1]\n",
        "p.get_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-i8NeLoVc9a"
      },
      "source": [
        "## Finding all instances of a tag at once\n",
        "\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "soup.find_all('p')\n",
        "soup.find_all('p')[0].get_text()\n",
        "\n",
        "soup.find('p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k_w6nMuVc9b"
      },
      "source": [
        "## Searching for tags by class and id\n",
        "\n",
        "page = requests.get(\"https://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "soup\n",
        "\n",
        "soup.find_all('p', class_='outer-text')\n",
        "soup.find_all(class_=\"outer-text\")\n",
        "soup.find_all(id=\"first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKNqWi-EVc9b"
      },
      "source": [
        "## Start Scraping!\n",
        "## We now know enough to download the page and start parsing it. In the below code, we will:\n",
        "# Download the web page containing the forecast.\n",
        "# Create a BeautifulSoup class to parse the page.\n",
        "# Find the div with id seven-day-forecast, and assign to seven_day\n",
        "# Inside seven_day, find each individual forecast item.\n",
        "# Extract and print the first forecast item.\n",
        "\n",
        "page = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=37.7772&lon=-122.4168\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "seven_day = soup.find(id=\"seven-day-forecast\")\n",
        "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
        "tonight = forecast_items[0]\n",
        "print(tonight.prettify())\n",
        "\n",
        "\n",
        "period = tonight.find(class_=\"period-name\").get_text()\n",
        "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
        "temp = tonight.find(class_=\"temp\").get_text()\n",
        "print(period)\n",
        "print(short_desc)\n",
        "print(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9wmNCKFVc9c"
      },
      "source": [
        "\n",
        "img = tonight.find(\"img\")\n",
        "desc = img['title']\n",
        "print(desc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebDAmDBMVc9c"
      },
      "source": [
        "# Extracting information from the page\n",
        "# As we can see, inside the forecast item tonight is all the information we want. There are four pieces of information we can extract:\n",
        "# The name of the forecast item — in this case, Tonight.\n",
        "# The description of the conditions — this is stored in the title property of img.\n",
        "# A short description of the conditions — in this case, Mostly Clear.\n",
        "# The temperature low — in this case, 49 degrees.\n",
        "\n",
        "period = tonight.find(class_=\"period-name\").get_text()\n",
        "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
        "temp = tonight.find(class_=\"temp\").get_text()\n",
        "print(period)\n",
        "print(short_desc)\n",
        "print(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1goauk-dVc9d"
      },
      "source": [
        "# Extracting all the information from the page\n",
        "# Select all items with the class period-name inside an item with the class tombstone-container in seven_day.\n",
        "# Use a list comprehension to call the get_text method on each BeautifulSoup object.\n",
        "\n",
        "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
        "periods = [pt.get_text() for pt in period_tags]\n",
        "periods"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzSZWuSnVc9d"
      },
      "source": [
        "# apply the same technique to get the other three fields\n",
        "\n",
        "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
        "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
        "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
        "print(short_descs)\n",
        "print(temps)\n",
        "print(descs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I4ZHMtEVc9d"
      },
      "source": [
        "# Combining our data into a Pandas Dataframe\n",
        "# call the DataFrame class, and pass in each list of items. \n",
        "# pass them in as part of a dictionary.\n",
        "#  Each dictionary key will become a column in the DataFrame \n",
        "# each list will become the values in the column\n",
        "\n",
        "import pandas as pd\n",
        "weather = pd.DataFrame({\n",
        "    \"period\": periods,\n",
        "    \"short_desc\": short_desc,\n",
        "    \"temp\": temp,\n",
        "    \"desc\":desc\n",
        "})\n",
        "weather"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PmHiSkDVc9e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}